{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.2 - IMDB Ratings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM/MAXXWELFuf0ERdk5AQ1U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Let's first load some libraries... "],"metadata":{"id":"SO8TJBMQiWH-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4X8iKJeh_ew"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tensorflow import keras \n","from tensorflow.keras import layers"]},{"cell_type":"markdown","source":["#**IMDB Dataset**"],"metadata":{"id":"z42A2dzeif1q"}},{"cell_type":"markdown","source":["We load the IMDB dataset, keeping only the 10,000 most frequent terms in the corpus. Each of those 10,000 terms is represented in the data by a unique integer in the range between 0 and 9,999. Each observation in the train or test data is therefore just a list of integer values. The associated labels are just binary indicators of positive or negative rating valence. IMDB ratings are out of 10, so I presume positive means something like 8+, and negative means something like < 8.  "],"metadata":{"id":"h0LngQpliCz1"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import imdb\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n","    num_words=10000)"],"metadata":{"id":"lEEsyJ68iijJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The observation in the training data with the largest number of 'frequent' terms has ~2,500 of them. Note that this is not the same thing as the number of words in the review. *Q: Why not?*\n","\n","Further, if we extract the largest integer value from each review in the training data, put it into a list, and then take the max of that list, we see the largest integer index is 9,999 (as expected). "],"metadata":{"id":"4HQYWYEBiS_l"}},{"cell_type":"code","source":["print(max(len(i) for i in train_data))\n","print(max([max(sequence) for sequence in train_data]))"],"metadata":{"id":"5CFcIBCkiXnP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can reverse the integer coding like this... "],"metadata":{"id":"tXVk8HrZlcWi"}},{"cell_type":"code","source":["# Here is the key-value dictionary that stores the integer indexes for each term.\n","# Note that the word index dictionary has 'garbage' values in the first three entries.\n","# These are explained in the dataset description if you look for it; they are special terms in the data.\n","word_index = imdb.get_word_index()\n","word_index[\"<PAD>\"] = 0\n","word_index[\"<START>\"] = 1\n","word_index[\"<UNK>\"] = 2\n","word_index[\"<UNUSED>\"] = 3\n","\n","# So, the word 'big' is represented by integer 191.\n","print(word_index.get(\"big\"))\n","\n","# We can reverse the keys and values for each entry, to get a dictionary that would let us 'decode' the terms from a review.\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","print(reverse_word_index)\n","\n","# Now we can convert integer values back to terms for a given review in the sample. \n","decoded_review = \" \".join([reverse_word_index.get(i) for i in train_data[0]])\n","print(train_data[0])\n","print(decoded_review)"],"metadata":{"id":"xppFEWRblg1x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#*Pre-processing the Data*"],"metadata":{"id":"Y50PoSvKSrTR"}},{"cell_type":"markdown","source":["Now, we need to pre-process the data. We can't just pass integer lists of variable length into the neural network. We need a fixed number of features to serve as our x's (inputs) to the network, i.e., same number of features for each observation in the training data (and also the test data for validation, later). The most obvious thing we can do is multi-hot encode the observations (i.e., dummy code it). So, we basically make a matrix with 10,000 columns, and set values to 1 for columns representing the terms we have and 0 for columns representing the terms we don't have. "],"metadata":{"id":"kXSMHDVwn99B"}},{"cell_type":"code","source":["import numpy as np\n","\n","def vectorize_sequences(sequences, dimension=10000): \n","    \n","    # Make our blank matrix of 0's to store hot encodings.\n","    results = np.zeros((len(sequences), dimension))\n","\n","    # For each observation and element in that observation,\n","    # Update the blank matrix to a 1 at row obs, column element value.\n","    for i, sequence in enumerate(sequences):\n","        for j in sequence:\n","            results[i, j] = 1.\n","    return results\n","\n","# I am converting the resulting giant arrays into float datatype. They are 'float64' by default, which takes more RAM.\n","x_train = vectorize_sequences(train_data).astype('float')\n","x_test = vectorize_sequences(test_data).astype('float')\n","\n","# Labels are already fine, but we can convert them to floats to match the x's data type (they are 'float32' by default.)\n","y_train = train_labels.astype('float')\n","y_test = test_labels.astype('float')"],"metadata":{"id":"GYjf3d0qqMZN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the hot-encoding looks correct:"],"metadata":{"id":"kdhdopNKRPA_"}},{"cell_type":"code","source":["print(train_data[0])\n","print(x_train[0,14])\n","print(x_train[0,22])\n","print(x_train[0,23])"],"metadata":{"id":"w0iGIC0s497c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#*Building the Model*"],"metadata":{"id":"yO1iPvOtSmrF"}},{"cell_type":"markdown","source":["So, mapping binary inputs to binary outputs is a very simple problem setup. We don't even need to whiten the data; features are already in the 0-1 range. We will follow the book's advice and make a bunch of dense layers with relu activations, followed by a sigmoid activated output layer. "],"metadata":{"id":"Owery7sTSwqa"}},{"cell_type":"code","source":["# Can install the tensorflow-addons package in your colab runtime. \n","try:\n","    import tensorflow_addons as tfa                     \n","except ImportError:\n","    !pip install tensorflow-addons\n","    import tensorflow_addons as tfa "],"metadata":{"id":"ajfM0eH_prmH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Some options for mitigating over-fitting:\n","#   - Weight regularization.\n","#   - Activation regularization.\n","#   - Weight constraints. \n","#   - Dropout\n","#   - Topology simplification\n","#   - Add noise to the input data. \n","\n","model = keras.Sequential([\n","    #layers.Dropout(0.2), # This layer sets a random fraction of weights to 0 in a given training pass. \n","    #layers.GaussianNoise(0.1), # This layer adds random normal noise to the input features.\n","    #tfa.layers.NoisyDense(16,activation=\"relu\"), # This injects noise into the weights at each step. \n","    layers.Dense(16, activation=\"relu\"), #,kernel_regularizer='l2'\n","    layers.Dense(16, activation=\"relu\"), #,activity_regularizer='l2'\n","    layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","# We will keep records 0-9,999 for validation, and we will use the remaining records for training.\n","# We are still holding out the test dataset by the way, so we are going to do train -> validation to figure out when overfitting happens.\n","# Then we are going to re-train on the whole training dataset with early stopping. Finally, we will evaluate performance on the test dataset.\n","x_val = x_train[:10000]\n","partial_x_train = x_train[10000:]\n","y_val = y_train[:10000]\n","partial_y_train = y_train[10000:]\n","\n","history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val))"],"metadata":{"id":"CEc0CrmVTJvd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, the model starts to overfit after 3-4 epochs; validation accuracy starts to decline. Here we are plotting loss..."],"metadata":{"id":"USKrU83MWZ3f"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","history_dict = history.history\n","loss_values = history_dict[\"loss\"]\n","val_loss_values = history_dict[\"val_loss\"]\n","epochs = range(1, len(loss_values) + 1)\n","plt.plot(epochs, loss_values, \"r\", label=\"Training loss\")\n","plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n","plt.title(\"Training and validation loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"nfqSeol6WCzb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we are plotting accuracy."],"metadata":{"id":"sAW9hq0V3NRS"}},{"cell_type":"code","source":["acc = history_dict[\"accuracy\"]\n","val_acc = history_dict[\"val_accuracy\"]\n","plt.plot(epochs, acc, \"r\", label=\"Training acc\")\n","plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n","plt.title(\"Training and validation accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ZwxfjENiWTiv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note: if you see your RAM creeping up, you might want to clear some things out of memory to free up space... the garbage collector can help here. I'm going to delete the existing model and the original test / train datasets (we have them in hot-encoded format now anyway. Or, you can set the objects to 0."],"metadata":{"id":"2lbGSXlqGoUE"}},{"cell_type":"code","source":["import gc\n","del model, train_data, test_data\n","gc.collect()\n","\n","# Setting the objects to empty can also help.\n","train_data, test_data = [],[]"],"metadata":{"id":"Df74EEy_Guaj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's re-run this and force a stop after 4 epochs. Note, you need to redefine the model here, else it will just pickup where your model left off (i.e., the last set of weights you were using, which are already overfit).\n","\n"],"metadata":{"id":"AZ-h9OetXBI7"}},{"cell_type":"code","source":["model = keras.Sequential([\n","    layers.Dense(16, activation=\"relu\"),\n","    layers.Dense(16, activation=\"relu\"),\n","    layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(optimizer=\"adam\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","history = model.fit(x_train,\n","                    y_train,\n","                    epochs=4,\n","                    batch_size=512,\n","                    validation_data=(x_test, y_test))\n","\n","# This will return our loss and accuracy metrics.\n","model.evaluate(x_test, y_test)"],"metadata":{"id":"qe-vr5H6XFqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And, of course, the model is useless if we can't use it to produce a new prediction. So, let's do that as well. "],"metadata":{"id":"_qKJZ6vT5iNo"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# y_test is a 1D numpy array, whereas our predictions are a 2D array.\n","print(y_test.shape)\n","\n","predictions = model.predict(x_test)\n","print(predictions.shape)\n","\n","# np.ravel() is another way to flatten an array into 1D, so then we can take the cross-tabulation.\n","# Note that I'm applying the >0.5 threshold rule to the predictions. \n","pd.crosstab(np.ravel(predictions)>0.5,y_test)"],"metadata":{"id":"6b5ONGqfDSFR"},"execution_count":null,"outputs":[]}]}