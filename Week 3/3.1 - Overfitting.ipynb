{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.1 - Overfitting.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfNXJdHWGmiCO6CwxPBSWz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Extreme Overfitting Example**"],"metadata":{"id":"_v-ehB4GqMFp"}},{"cell_type":"markdown","source":["I'm going to simulate a set of random images and their associated labels. I want you to then fit a neural network to the random data. Finally, report how accurate the model is on the training data, and visualize training accuracy across epochs. "],"metadata":{"id":"5z4RvBH2qS-F"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HO7zhZ1ep2FJ"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from tensorflow import keras \n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","from PIL import Image as im\n","\n","# Let's synthesize 20,000 images comprised of random pixel values.\n","train_images_random = np.random.random((5000, 28,28))\n","df = pd.DataFrame(train_images_random.reshape(-1)) #-1 is a wildcard; it'll guess what shape I want, and by default give a vector back.\n","print(f'Descriptive statistics for the training data:\\n{df.describe()}\\n')\n","\n","# And let's make-up labels for them. \n","train_labels_random = np.floor(np.random.random(5000)*10)\n","print(f'The first ten labels are {train_labels_random[:10]}\\n')\n","\n","# Here's what the first picture looks like.\n","plt.imshow(train_images_random[0],cmap=plt.cm.binary)\n","plt.show()\n","\n","# Let's first convert each pixel matrix into a vector. \n","train_images_random = train_images_random.reshape(5000,28*28)\n","train_labels_random = train_labels_random.reshape(5000,1)\n","print(train_images_random.shape)\n","print(train_labels_random.shape)"]},{"cell_type":"markdown","source":["Now we will use the simulated data and fit a sequential Keras model that has 512 units in the first layer, with a Relu activation, and then the output layer. "],"metadata":{"id":"iCydBpG9qtIX"}},{"cell_type":"code","source":["# Setup your model topology here using the Sequential API.\n","model = keras.Sequential([\n","      layers.Dense(512,activation=\"relu\"),\n","      layers.Dense(10, activation=\"softmax\")                         \n","])\n","\n","# Compile your model\n","model.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n","\n","# Fit your model and store the training history in a variable (history) that you can query to plot the training loss, later. \n","history = model.fit(train_images_random,train_labels_random, batch_size=25,epochs=30)"],"metadata":{"id":"kaFbloFFsg7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, plot the training loss now... "],"metadata":{"id":"R_6DF1bevD8k"}},{"cell_type":"code","source":["# Call plot commands here. \n","plt.plot(history.history['loss'])\n","plt.show()"],"metadata":{"id":"72-YP3FCvITT"},"execution_count":null,"outputs":[]}]}