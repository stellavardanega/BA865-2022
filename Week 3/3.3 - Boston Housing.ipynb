{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LlY8FxDj_GJJ"},"outputs":[],"source":["from tensorflow.keras.datasets import boston_housing\n","(train_data, train_targets), (test_data, test_targets) = (\n","    boston_housing.load_data())"]},{"cell_type":"markdown","metadata":{"id":"FMEdXpaO_8Tc"},"source":["We don't have a very big dataset here... it's just 506 rows, total, 13 features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vTVWnOF_0Y9"},"outputs":[],"source":["print(\"Train data dimensions:\",train_data.shape)\n","print(\"Test data dimensions:\",test_data.shape)\n","print(train_targets) # these are home prices in $1,000s of dollars (in the 70s that was a lot of money)"]},{"cell_type":"markdown","metadata":{"id":"5JbG2s9SAVRK"},"source":["Let's 'whiten' the data..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aST1M3scAU1b"},"outputs":[],"source":["# We can white the data like this; or we can use the np.mean(), np.sub(), and np.divide() functions.\n","mean = train_data.mean(axis=0)\n","train_data -= mean\n","std = train_data.std(axis=0)\n","train_data /= std\n","test_data -= mean\n","test_data /= std"]},{"cell_type":"markdown","metadata":{"id":"SAo9LHOIAYmU"},"source":["And, let's define our simple network. We are going to define a simple function that re-specifies a model and compiles it. We're doing this because we will use this inside a nested loop that implements k-fold cross-validation. This is something you've probably already learned in other classes, but when your sample is small, it's generally not a good idea to rely on single random holdout for model evaluation. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzx0nWWwAbMu"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def build_model():\n","    model = keras.Sequential([\n","        layers.Dropout(0.1),\n","        layers.BatchNormalization(), # This whitens the inputs to the next layer (de-mean, divide by SD, which can help with training)\n","        layers.Dense(8, activation=\"relu\"),\n","        layers.Dropout(0.2),\n","        layers.BatchNormalization(),\n","        layers.Dense(4, activation=\"relu\"),\n","        layers.Dense(1)\n","    ])\n","    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"e41aeQ4KB6h7"},"source":["The example code from the book breaks up the training sample into k folds, and cycles over each, leveraging the *other* k-1 folds for training, and using the focal fold evaluation. We store the average validation error metric across the k runs. We can repeat this process using different numbers of epochs, batch sizes, etc. Eventually, when we settle on a model that looks good, we do a final evaluation on the test data.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaJuoT7QCHnY"},"outputs":[],"source":["import numpy as np\n","\n","k = 4 \n","num_val_samples = len(train_data) // k # floor division (i.e., round down to nearest integer.)\n","num_epochs = 500\n","all_mae_histories = []  \n","\n","print(\"In total, we have\",len(train_data),\"training observations.\")\n","print(\"With a k of\",k,\"we have\",num_val_samples,\"observations per fold.\\n\")\n","\n","for i in range(k): # the folds are going to be indexed 0 through 3 if k = 4\n","    print(\"Processing fold #:\",i)\n","    # if I slice past the end of the array, it just gives me what it can find! No errors.\n","    # This is important here, because the last fold won't produce an error, despite our slice going well beyond the end of the array.\n","    print(\"Validation data includes observations\",i*num_val_samples,\"through\",(i+1)*num_val_samples-1) # minus 1 because a slice is up to and not including the second index.\n","    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] \n","    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","    print(\"Training data includes observations 0 through\",i*num_val_samples-1,\"joined with observations\",(i+1)*num_val_samples,\"through the final observation.\\n\")\n","    partial_train_data = np.concatenate(\n","        [train_data[:i * num_val_samples],\n","         train_data[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    partial_train_targets = np.concatenate(\n","        [train_targets[:i * num_val_samples],\n","         train_targets[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    model = build_model()\n","    history = model.fit(partial_train_data, partial_train_targets,\n","                        validation_data=(val_data, val_targets),\n","                        epochs=num_epochs, batch_size=16, verbose=0)\n","    mae_history = history.history['val_mae']\n","    all_mae_histories.append(mae_history)"]},{"cell_type":"markdown","metadata":{"id":"bX7aHci0IqdY"},"source":["Now we can calculate the average MAE in each epoch, across the 4 iterations. That is, for epoch 1, take the average MAE over the 4 iterations, then do it again for epoch 2, and so on. We will end up with 500 averages.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMxYGrnTJEe8"},"outputs":[],"source":["average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n","len(average_mae_history)"]},{"cell_type":"markdown","metadata":{"id":"M3v4wWCEJ4Y-"},"source":["And now we plot them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-0GmU9dJ6D5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(average_mae_history[10:])\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Validation MAE\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HY_7rMNQKTXS"},"source":["Now we fit a final model and evaluate its performance on the test data. Being off by ~$2,500 in 70's dollars is a lot actually. Our model isn't that great. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKIk-xMuKV5Q"},"outputs":[],"source":["model = build_model()\n","model.fit(train_data, train_targets,\n","          epochs=150, batch_size=16, verbose=0)\n","test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"]},{"cell_type":"markdown","metadata":{"id":"hmFtFngZK9cM"},"source":["And of course, this is how we obtain predictions from the model. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqTnff_cK6ck"},"outputs":[],"source":["predictions = model.predict(test_data)\n","print(predictions)"]}],"metadata":{"colab":{"name":"3.3 - Boston Housing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNYtlH3qHwAs7ttDcnvVRVG"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"nbformat":4,"nbformat_minor":0}